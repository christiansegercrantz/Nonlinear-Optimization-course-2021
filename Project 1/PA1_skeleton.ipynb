{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MS-E2122 - Nonlinear Optimization\n",
        "### Prof. Fabricio Oliveira\n",
        "\n",
        "## Project Assignment 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "using ForwardDiff    \n",
        "using LinearAlgebra\n",
        "using BenchmarkTools\n",
        "using Test \n",
        "using Plots\n",
        "using LaTeXStrings \n",
        "pyplot() \n",
        "\n",
        "## Functions to compute gradient, hessian and first derivative\n",
        "∇(f,x) = ForwardDiff.gradient(f, x);\n",
        "H(f,x) = ForwardDiff.hessian(f, x);\n",
        "d(θ,λ) = ForwardDiff.derivative(θ, λ)\n",
        "\n",
        "const tol = 1e-3;      # unconstrained method convergence tolerance\n",
        "const tol_ls = 1e-7;   # line search convergence tolerance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exact line search : bisection method\n",
        "\n",
        "We will implement below the method that will be exact line search in our unconstrained optimisation methods. \n",
        "\n",
        "Notice that all our function will have a list the keyword arguments (those occuring AFTER the semi-colon ';') that already have standard values assigned. Two things about those:\n",
        "1. I (Fabricio) prefer to use keyword arguments because they are order independent (so you don't have to struggle with functions with arguments in the wrong order) and it makes it clear to the user what parameters are being set.\n",
        "2. If a keyword argument has a standard parameter assigned, setting the value of that argument when calling the function is optional. For example, if we are happy with the standard values, the function below can be called as `bisection(\\theta)`, without needing any further argument \n",
        "\n",
        "### Inputs: \n",
        "- θ: line search function\n",
        "- a: initial lower bound\n",
        "- b: initial upper bound\n",
        "\n",
        "### Output:\n",
        "- λ: step size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "bisection (generic function with 1 method)"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "function bisection(θ; a=0.0, b=10.0, ϵ=tol_ls)\n",
        "\n",
        "    λ, dθ = 0.0, 0.0\n",
        "    \n",
        "    while b - a > ϵ\n",
        "        λ = (a + b)/2\n",
        "        dθ= d(θ, λ)\n",
        "        \n",
        "        # TODO: implement the bisection method update\n",
        "        if dθ == 0 \n",
        "            return λ\n",
        "        elseif dθ > 0\n",
        "            b = λ\n",
        "        else\n",
        "            a = λ\n",
        "        end\n",
        "\n",
        "    end\n",
        "        return λ\n",
        "end"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Testing the exact line search \n",
        "\n",
        "Notice we are using the same \"test bed\" from homework 2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Test functions\n",
        "θ1(x) = 3x^2 -4x + 6           # Optimal value x = 2/3\n",
        "θ2(x) = exp(x) - 10x^2 - 20x   # Optimal value x = 4.743864 (in [0,10])\n",
        "\n",
        "@test abs(bisection(θ1) - 2/3) <= 1e-4\n",
        "@test abs(bisection(θ2) - 4.743864) <= 1e-4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Inexact line search : Armijo rule\n",
        "\n",
        "### Inputs: \n",
        "- θ: line search function\n",
        "- λ: initial step size value\n",
        "- α: slope reduction factor\n",
        "- β: λ reduction factor\n",
        "\n",
        "### Output:\n",
        "- λ: a step size value that satisfies the Armijo rule"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "armijo (generic function with 1 method)"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "function armijo(θ; λ=1.0, α=0.01, β=0.7) \n",
        "    \n",
        "    θ₀  = θ(0)                 # Function value at zero (use \\theta + tab and \\_0 + tab to add the subscript to θ)\n",
        "    dθ = d(θ, 0)               # Derivative (slope) at zero   \n",
        "    \n",
        "    # TODO: Implement what should be the while loop of Armijo method\n",
        "    \n",
        "    while θ(λ) > θ₀ + α*λ*dθ\n",
        "        λ = β*λ \n",
        "    end\n",
        "    \n",
        "    return λ\n",
        "end"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Testing the inexact line search \n",
        "\n",
        "Armijo returns pproximate values for the optimal. Notice for the second function we need to increase the initial step size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Test functions\n",
        "θ1(x) = 3x^2 -4x + 6           # Optimal value x = 2/3\n",
        "θ2(x) = exp(x) - 10x^2 - 20x   # Optimal value x = 4.743864 (in [0,10])\n",
        "\n",
        "@test abs(armijo(θ1) - 1.0) <= 1e-4\n",
        "@test abs(armijo(θ2, λ=20.0) - 4.802) <= 1e-4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Gradient (descent) method\n",
        "\n",
        "### Inputs\n",
        "- f: function to minimize\n",
        "- x_start: starting point\n",
        "- max_steps: maximum number of iterations\n",
        "- exact_ls: if `true`, the method uses the bisection line search; if `false`, it uses the Armijo rule \n",
        "- save_history: set `true` if you need the history of the iterations for analysing the trajectory.\n",
        "- ϵ: convergence tolerance \n",
        "\n",
        "### Outputs\n",
        "Either `(x_iter, k-1)` (if `save_history = true`) or `(f(x), k-1) `(if `save_history = false`)  \n",
        "- x_iter: a matrix with n columns and as many rows as iterations with each point visited \n",
        "- f(x): function value at the point of convergence\n",
        "- k-1: number of iterations, discounted the \"iteration 0\" for which k = 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "gradient (generic function with 1 method)"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "function gradient(f; x_start=[0,0], max_steps=1000, exact_ls=true, save_history=false, ϵ=tol)\n",
        "    \n",
        "    # if we need to save the history of iterations \n",
        "    if save_history \n",
        "        x_iter = zeros(max_steps, length(x_start))\n",
        "    end \n",
        "    \n",
        "    x = x_start               # initial x should be set to x_start \n",
        "    for k = 1:max_steps                \n",
        "        ∇f = ∇(f, x)          # Gradient at iteration k  \n",
        "        norm∇f = norm(∇f)     # Norm of the gradient        \n",
        "        \n",
        "        if norm∇f < ϵ         # Stopping condition #1           \n",
        "            if save_history\n",
        "                x_iter = vcat(x_start', x_iter)    # Including starting point\n",
        "                return (x_iter[1:k, :], k-1)       # Return the history of points, iterations            \n",
        "            else\n",
        "                return (f(x), k-1)                 # Return cost and iterations            \n",
        "            end                    \n",
        "        end\n",
        "        \n",
        "        # TODO: set the Gradient Descent direction d (don't forget the normalisation)\n",
        "        d = -∇f/norm(∇f)\n",
        "        \n",
        "        # TODO: define the line search function θ\n",
        "        θ(λ) = f(x + λ*d)\n",
        "\n",
        "        if exact_ls\n",
        "            λ = bisection(θ)    # Calls Golden Section method to compute optimal step size λ                         \n",
        "        else     \n",
        "            λ = armijo(θ)       # Calls Armijo method to compute optimal step size λ  \n",
        "        end\n",
        "       \n",
        "        # TODO: Update the solution x at this iteration accordingly\n",
        "        x = x + λ*d\n",
        "        \n",
        "        if save_history \n",
        "            x_iter[k, :] = x    # save the history if needed\n",
        "        end    \n",
        "    end\n",
        "    \n",
        "    if save_history\n",
        "        x_iter = vcat(x_start', x_iter)    # Including starting point\n",
        "        return (x_iter, max_steps)         # Return the history of cost, iterations\n",
        "    else    \n",
        "        return (f(x), max_steps)           # Return last cost, iterations count \n",
        "    end\n",
        "    \n",
        "end"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Testing the gradient method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Function fot the test \n",
        "f(x) = exp(x[1] + 2*x[2] - 0.2) + exp(x[1] - 2*x[2] - 0.2) + exp(-x[1] - 0.2) \n",
        "\n",
        "# Starting point for f\n",
        "x = [-4.0, -2.0]      \n",
        " \n",
        "# If your code is correct these should return the optimal point [-0.34657 0.0] and the optimal value 2.3157202\n",
        "(xg, kg) = gradient(f, x_start = x, exact_ls = true, save_history = true)\n",
        "\n",
        "@test abs(f(xg[end,:]) - 2.3157202) <= 1e-4\n",
        "@test norm(xg[end,:]) <= 1e04"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Measuring performance\n",
        "\n",
        "Julia offers the macro `@time` for measuring time, but it is more robust to use `@benchmark` and `@btime` from the package `BenchmarkTools`. In the former the funcion is executed multiple times and average values are reported.\n",
        "\n",
        "Notice that Julia uses something called just-in-time compilation. That roughly means that the first time you run a new function, there is a extra time for the code to be compiled. The macro `@time` informs the amount of time used in compilation, while `@btime` discard this measurement with compilation time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "BenchmarkTools.Trial: 10000 samples with 1 evaluation.\n",
              " Range \u001b[90m(\u001b[39m\u001b[36m\u001b[1mmin\u001b[22m\u001b[39m … \u001b[35mmax\u001b[39m\u001b[90m):  \u001b[39m\u001b[36m\u001b[1m53.300 μs\u001b[22m\u001b[39m … \u001b[35m 3.476 ms\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmin … max\u001b[90m): \u001b[39m0.00% … 97.72%\n",
              " Time  \u001b[90m(\u001b[39m\u001b[34m\u001b[1mmedian\u001b[22m\u001b[39m\u001b[90m):     \u001b[39m\u001b[34m\u001b[1m60.400 μs              \u001b[22m\u001b[39m\u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmedian\u001b[90m):    \u001b[39m0.00%\n",
              " Time  \u001b[90m(\u001b[39m\u001b[32m\u001b[1mmean\u001b[22m\u001b[39m ± \u001b[32mσ\u001b[39m\u001b[90m):   \u001b[39m\u001b[32m\u001b[1m66.261 μs\u001b[22m\u001b[39m ± \u001b[32m91.458 μs\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmean ± σ\u001b[90m):  \u001b[39m4.27% ±  3.08%\n",
              "\n",
              "  \u001b[39m█\u001b[39m▃\u001b[39m▁\u001b[39m \u001b[39m▁\u001b[34m▃\u001b[39m\u001b[39m \u001b[39m \u001b[39m \u001b[32m \u001b[39m\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \n",
              "  \u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m▇\u001b[39m█\u001b[34m█\u001b[39m\u001b[39m▇\u001b[39m▇\u001b[39m▆\u001b[32m▃\u001b[39m\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▃\u001b[39m▃\u001b[39m▂\u001b[39m▂\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m \u001b[39m▂\n",
              "  53.3 μs\u001b[90m         Histogram: frequency by time\u001b[39m         140 μs \u001b[0m\u001b[1m<\u001b[22m\n",
              "\n",
              " Memory estimate\u001b[90m: \u001b[39m\u001b[33m50.72 KiB\u001b[39m, allocs estimate\u001b[90m: \u001b[39m\u001b[33m899\u001b[39m."
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Obtaining time statistics using @benchmark\n",
        "@benchmark gradient(f, x_start = x, exact_ls = true)    # Gradient method with exact line search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gradient method with exact line search:  54.400 μs (899 allocations: 50.69 KiB)\n",
            "Gradient method with Armijo line search:  63.900 μs (1785 allocations: 82.75 KiB)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(2.3157203731087854, 22)"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Since we are only interested in the time, we can use @btime instead\n",
        "print(\"Gradient method with exact line search:\")\n",
        "@btime gradient(f, x_start = x)    # Gradient method with exact line search\n",
        "\n",
        "print(\"Gradient method with Armijo line search:\")\n",
        "@btime gradient(f, x_start = x, exact_ls = false)   # Gradient method with inexact line search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gradient method with exact line search:  0.000093 seconds (899 allocations: 50.688 KiB)\n",
            "Gradient method with Armijo line search:  0.000106 seconds (1.78 k allocations: 82.750 KiB)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(2.3157203731087854, 22)"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Or if we want to measure time once, we can use @time\n",
        "print(\"Gradient method with exact line search:\")\n",
        "@time gradient(f, x_start = x)    # Gradient method with exact line search\n",
        "\n",
        "print(\"Gradient method with Armijo line search:\")\n",
        "@time gradient(f, x_start = x, exact_ls = false)   # Gradient method with inexact line search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Heavy ball method\n",
        "\n",
        "### Inputs\n",
        "- f: function to minimize\n",
        "- weight: Heavy ball weighting parameter\n",
        "- x_start: starting point\n",
        "- max_steps: maximum number of iterations\n",
        "- exact_ls: if `true`, the method uses the bisection line search; if `false`, it uses the Armijo rule \n",
        "- save_history: set `true` if you need the history of the iterations for analysing the trajectory.\n",
        "- ϵ: convergence tolerance \n",
        "\n",
        "### Outputs\n",
        "Either `(x_iter, k-1)` (if `save_history = true`) or `(f(x), k-1) `(if `save_history = false`)  \n",
        "- x_iter: a matrix with n columns and as many rows as iterations with each point visited \n",
        "- f(x): function value at the point of convergence\n",
        "- k-1: number of iterations, discounted the \"iteration 0\" for which k = 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "heavy_ball (generic function with 1 method)"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "function heavy_ball(f; weight=0.5, x_start=[0,0], max_steps=1000, exact_ls=true, save_history=false, ϵ=tol)\n",
        "    \n",
        "    # If we need to save the history of iterations \n",
        "    if save_history \n",
        "        x_iter = zeros(max_steps, length(x_start))\n",
        "    end \n",
        "    \n",
        "    d = zeros(size(x_start))  # Placeholder for direction d\n",
        "    \n",
        "    x = x_start               # Initial x should be set to x_start \n",
        "    for k = 1:max_steps                \n",
        "        ∇f = ∇(f, x)          # Gradient at iteration k  \n",
        "        norm∇f = norm(∇f)     # Norm of the gradient        \n",
        "        \n",
        "        if norm∇f < ϵ         # Stopping condition #1           \n",
        "            if save_history\n",
        "                x_iter = vcat(x_start', x_iter)    # Including starting point\n",
        "                return (x_iter[1:k, :], k-1)     # Return the history of points, iterations            \n",
        "            else\n",
        "                return (f(x), k-1)                 # Return cost and iterations            \n",
        "            end                    \n",
        "        end     \n",
        "        \n",
        "        # TODO: set the Heavy ball direction d\n",
        "        d = -(1-weight)*∇f/norm(∇f) + weight*d\n",
        "        \n",
        "        # TODO: define the line search function θ\n",
        "        θ(λ) = f(x + λ*d)\n",
        "\n",
        "        if exact_ls\n",
        "            λ = bisection(θ)    # Calls Golden Section method to compute optimal step size λ                         \n",
        "        else     \n",
        "            λ = armijo(θ)       # Calls Armijo method to compute optimal step size λ  \n",
        "        end\n",
        "              \n",
        "        # TODO: Update the solution x at this iteration accordingly\n",
        "        x = x + λ*d\n",
        "        \n",
        "        if save_history \n",
        "            x_iter[k, :] = x    # save the history if needed\n",
        "        end \n",
        "    end\n",
        "    \n",
        "    if save_history\n",
        "        x_iter = vcat(x_start', x_iter)    # Including starting point\n",
        "        return (x_iter, max_steps)  # Return the history of cost, iterations\n",
        "    else    \n",
        "        return (f(x), max_steps)    # Return last cost, iterations count \n",
        "    end\n",
        "\n",
        "end"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Testing the heavy ball method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Function fot the test \n",
        "f(x) = exp(x[1] + 2*x[2] - 0.2) + exp(x[1] - 2*x[2] - 0.2) + exp(-x[1] - 0.2) \n",
        "\n",
        "# Starting point for f\n",
        "x = [-4.0, -2.0]      \n",
        " \n",
        "# If your code is correct these should return the optimal point [-0.34657 0.0] and the optimal value 2.3157202\n",
        "(xh, kh) = heavy_ball(f, x_start = x, exact_ls = true, save_history = true)\n",
        "\n",
        "@test abs(f(xh[end,:]) - 2.3157202) <= 1e-4\n",
        "@test norm(xh[end,:]) <= 1e04"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Time stats for heavy ball method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Heavy ball method with exact line search (default weight = 0.7):  112.600 μs (2161 allocations: 113.80 KiB)\n",
            "Heavy ball method with exact line search and weight = 0.3:  75.700 μs (1446 allocations: 76.09 KiB)\n",
            "Heavy ball method with inexact line search and default weight:  58.600 μs (1687 allocations: 76.78 KiB)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(2.315720336333798, 23)"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Obtaining time statistics using @btime \n",
        "print(\"Heavy ball method with exact line search (default weight = 0.7):\")\n",
        "@btime heavy_ball(f, x_start = x)                  # Heavy ball method with exact line search\n",
        "\n",
        "print(\"Heavy ball method with exact line search and weight = 0.3:\")\n",
        "@btime heavy_ball(f, weight = 0.3, x_start = x)    # Heavy ball method with exact line search and different weight\n",
        "\n",
        "print(\"Heavy ball method with inexact line search and default weight:\")\n",
        "@btime heavy_ball(f, x_start = x, exact_ls = false)                 # Heavy ball method with inexact line search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Newton's method\n",
        "\n",
        "### Inputs\n",
        "- f: function to minimize\n",
        "- x_start: starting point\n",
        "- max_steps: maximum number of iterations\n",
        "- exact_ls: if `true`, the method uses the bisection line search; if `false`, it uses the Armijo rule \n",
        "- save_history: set `true` if you need the history of the iterations for analysing the trajectory.\n",
        "- ϵ: convergence tolerance \n",
        "\n",
        "### Outputs\n",
        "Either `(x_iter, k-1)` (if `save_history = true`) or `(f(x), k-1) `(if `save_history = false`)  \n",
        "- x_iter: a matrix with n columns and as many rows as iterations with each point visited \n",
        "- f(x): function value at the point of convergence\n",
        "- k-1: number of iterations, discounted the \"iteration 0\" for which k = 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "newton (generic function with 1 method)"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "function newton(f; x_start=[0,0], max_steps=1000, exact_ls=true, save_history=false, ϵ=tol, true_inverse = true) \n",
        "    \n",
        "    # if we need to save the history of iterations \n",
        "    if save_history \n",
        "        x_iter = zeros(max_steps, length(x_start))\n",
        "    end \n",
        "    \n",
        "    x = x_start               # initial x should be set to x_start \n",
        "    for k = 1:max_steps                \n",
        "        ∇f = ∇(f, x)          # Gradient at iteration k  \n",
        "        norm∇f = norm(∇f)     # Norm of the gradient        \n",
        "    \n",
        "        if norm∇f < ϵ         # Stopping condition #1           \n",
        "            if save_history\n",
        "                x_iter = vcat(x_start', x_iter)    # Including starting point\n",
        "                return (x_iter[1:k, :], k-1)     # Return the history of points, iterations            \n",
        "            else\n",
        "                return (f(x), k-1)                 # Return cost and iterations            \n",
        "            end                    \n",
        "        end     \n",
        "        \n",
        "        # TODO: set the Newton's method direction d (remeber that backslash '\\' is more efficient than inv())\n",
        "        Hf = -H(f, x)\n",
        "        if true_inverse\n",
        "            d = inv(Hf)*∇f\n",
        "        else\n",
        "            d = Hf\\∇f\n",
        "        end\n",
        "        \n",
        "        # TODO: define the line search function θ\n",
        "        θ(λ) = f(x + λ*d)\n",
        "\n",
        "        if exact_ls\n",
        "            λ = bisection(θ)    # Calls Golden Section method to compute optimal step size λ                         \n",
        "        else     \n",
        "            λ = armijo(θ)       # Calls Armijo method to compute optimal step size λ  \n",
        "        end\n",
        "       \n",
        "        # TODO: Update the solution x at this iteration accordingly\n",
        "        x = x + λ*d\n",
        "        \n",
        "        if save_history \n",
        "            x_iter[k, :] = x    # save the history if needed\n",
        "        end \n",
        "    end\n",
        "    \n",
        "    if save_history\n",
        "        x_iter = vcat(x_start', x_iter)    # Including starting point\n",
        "        return (x_iter, max_steps)  # Return the history of cost, iterations\n",
        "    else    \n",
        "        return (f(x), max_steps)    # Return last cost, iterations count \n",
        "    end      \n",
        "end"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tests for the Newton's method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Function for the tests \n",
        "f(x) = exp(x[1] + 2*x[2] - 0.2) + exp(x[1] - 2*x[2] - 0.2) + exp(-x[1] - 0.2) \n",
        "\n",
        "# Starting point for f\n",
        "x = [-4.0, -2.0]      \n",
        " \n",
        "# If your code is correct these should return the optimal point [-0.34657 0.0] and the optimal value 2.3157202\n",
        "(xn, kn) = newton(f, x_start = x, exact_ls = true, save_history = true)\n",
        "\n",
        "@test abs(f(xn[end,:]) - 2.3157202) <= 1e-4\n",
        "@test norm(xn[end,:]) <= 1e04"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Time stats for the Newton's method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Newton with exact line search:  45.800 μs (764 allocations: 47.39 KiB)\n",
            "Newton with inexact line search:  23.300 μs (258 allocations: 24.86 KiB)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(2.315720269899818, 6)"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Obtaining time statistics using @btime \n",
        "print(\"Newton with exact line search:\")\n",
        "@btime newton(f, x_start = x)\n",
        "\n",
        "print(\"Newton with inexact line search:\")\n",
        "@btime newton(f, x_start = x, exact_ls = false)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Broyden–Fletcher–Goldfarb–Shanno (BFGS) method \n",
        "\n",
        "\n",
        "### Inputs\n",
        "- f: function to minimize\n",
        "- x_start: starting point\n",
        "- max_steps: maximum number of iterations\n",
        "- exact_ls: if `true`, the method uses the bisection line search; if `false`, it uses the Armijo rule \n",
        "- save_history: set `true` if you need the history of the iterations for analysing the trajectory.\n",
        "- ϵ: convergence tolerance \n",
        "\n",
        "### Outputs\n",
        "Either `(x_iter, k-1)` (if `save_history = true`) or `(f(x), k-1) `(if `save_history = false`)  \n",
        "- x_iter: a matrix with n columns and as many rows as iterations with each point visited \n",
        "- f(x): function value at the point of convergence\n",
        "- k-1: number of iterations, discounted the \"iteration 0\" for which k = 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "function bfgs(f; x_start=[0,0], max_steps=1000, exact_ls=true, save_history=false, ϵ=tol, true_inverse = true) \n",
        "    \n",
        "    # if we need to save the history of iterations \n",
        "    if save_history \n",
        "        x_iter = zeros(max_steps, length(x_start))\n",
        "    end \n",
        "    \n",
        "    n = length(x_start)       # Number of variables in the problem\n",
        "    B  = I(n)                 # Initial Hessian approximation is the identity\n",
        "\n",
        "    x = x_start               # initial x should be set to x_start \n",
        "    for k = 1:max_steps                \n",
        "        ∇f = ∇(f, x)          # Gradient at iteration k  \n",
        "        norm∇f = norm(∇f)     # Norm of the gradient        \n",
        "        # println(norm∇f)\n",
        "        if norm∇f < ϵ         # Stopping condition #1           \n",
        "            if save_history\n",
        "                x_iter = vcat(x_start', x_iter)    # Including starting point\n",
        "                return (x_iter[1:k, :], k-1)     # Return the history of points, iterations            \n",
        "            else\n",
        "                return (f(x), k-1)                 # Return cost and iterations            \n",
        "            end                    \n",
        "        end\n",
        "        \n",
        "        # TODO: set the BFGS method direction d. Be consistent wiht the update used, \n",
        "        # that is, either the matrix B or its inverse. The latter doesn not require any backslash `/`\n",
        "        # or inv() - do not use inv() though, for the same reason as in the Newton's method. \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # TODO: define the line search function θ\n",
        "\n",
        "\n",
        "        if exact_ls\n",
        "            λ = bisection(θ)    # Calls Golden Section method to compute optimal step size λ                         \n",
        "        else     \n",
        "            λ = armijo(θ)       # Calls Armijo method to compute optimal step size λ  \n",
        "        end \n",
        "        \n",
        "        p = λ * d                            # p = step size * direction\n",
        "       \n",
        "        # TODO: Update the solution x at this iteration accordingly\n",
        "\n",
        "        \n",
        "        if save_history \n",
        "            x_iter[k, :] = x    # save the history if needed\n",
        "        end \n",
        "        \n",
        "        ∇fn   = ∇(f, x)                      # New gradient\n",
        "        q     = ∇fn - ∇f                     # Update Gradient difference\n",
        "        ∇f    = ∇fn                          # Update Gradient for next iteration\n",
        "        \n",
        "        # TODO: Update the approximation of the of the Hessian (or its inverse)\n",
        "\n",
        "\n",
        "\n",
        "       \n",
        "    end\n",
        "    \n",
        "    if save_history\n",
        "        x_iter = vcat(x_start', x_iter)    # Including starting point\n",
        "        return (x_iter, max_steps)  # Return the history of cost, iterations\n",
        "    else    \n",
        "        return (f(x), max_steps)    # Return last cost, iterations count \n",
        "    end\n",
        "\n",
        "end"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tests for the BFGS method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function fot the test \n",
        "f(x) = exp(x[1] + 2*x[2] - 0.2) + exp(x[1] - 2*x[2] - 0.2) + exp(-x[1] - 0.2) \n",
        "\n",
        "# Starting point for f\n",
        "x = [-4.0, -2.0]      \n",
        " \n",
        "# If your code is correct these should return the optimal point [-0.34657 0.0] and the optimal value 2.3157202\n",
        "(xb, kb) = bfgs(f, x_start = x, exact_ls = true, save_history = true)\n",
        "\n",
        "@test abs(f(xb[end,:]) - 2.3157202) <= 1e-4\n",
        "@test norm(xb[end,:]) <= 1e04"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Times stats for the BFGS method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Obtaining time statistics using @btime \n",
        "print(\"BFGS with exact line search:\")\n",
        "@btime bfgs(f, x_start = x)\n",
        "\n",
        "print(\"BFGS with inexact line search:\")\n",
        "@btime bfgs(f, x_start = x, exact_ls = false)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plotting trajectories\n",
        "\n",
        "As a quick refresher, here is an example code for plotting the trajectories of each method. A more complete example is given in Homework 2. Also, make sure you consult the docuemntation of `Plots.jl` [(link here)](http://docs.juliaplots.org/latest/) to obtain the plots you would like."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Using the same function and starting point from the tests\n",
        "f(x) = exp(x[1] + 2*x[2] - 0.2) + exp(x[1] - 2*x[2] - 0.2) + exp(-x[1] - 0.2)\n",
        "x = [-1.5, -2.0]  \n",
        "\n",
        "# NOTE: Don't forget to set the keyword argument save_history parameter to 'true' implying that you want to receive the history of the iterations as an output\n",
        "# Gather the history of the iterations for the Gradient method with Armijo rule as an inexact line search\n",
        "(xga, kga) = gradient(f, x_start=x, exact_ls=false, save_history=true)\n",
        "\n",
        "# Gather the history of the iterations for the Gradient method with Golden section rule as an exact line search\n",
        "(xgg, kgg) = gradient(f, x_start=x, save_history=true)\n",
        "\n",
        "# Defining the interval for the x1(x) and x2(y) axes\n",
        "x = range(-5, 5, length = 100)\n",
        "y = copy(x)\n",
        "\n",
        "# Plotting the contour of the function \n",
        "contour(x,y, (x,y) -> f([x,y]), \n",
        "    title  = \"Test function\",\n",
        "    levels = [0.0 + i for i = 1:20],  # which level curves to show \n",
        "    cbar = false,     \n",
        "    clims = (0,20),                   # adjust the coloring limits\n",
        "    xaxis = (L\"$x_1$\", (-2.5, 1.5)),  # the L at the front let us use LaTeX axis labels; the parethesis give the visual range \n",
        "    yaxis = (L\"$x_2$\", (-2.5, 1.5)),\n",
        ")\n",
        "\n",
        "# Plotting the trajectory of the Gradient method with Armijo rule as an inexact line search\n",
        "plot!(xga[:,1], xga[:,2], label = \"Gradient (Armijo)\", marker=:circle)\n",
        "\n",
        "# Plotting the trajectory of the Gradient method with Golden section rule as an exact line search\n",
        "plot!(xgg[:,1], xgg[:,2], label = \"Gradient (exact)\", marker=:circle)\n",
        "\n",
        "# Saving the figure as .pdf file if needed\n",
        "savefig(\"test_function_gradient.pdf\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Space for answering Tasks (1) - (3)\n",
        "\n",
        "The above has everything you need to answer the qeustion posed in Tasks (1)-(3). Be organised with your code, and comment well what you have implemented, so I can undersand what you are trying to achieve. Use the cells below to generate any comparisons and or plots you might want to include in your report. Make sure your arguments are supported by data and analysses obtained from these. And, if you need help with anything, ask on the Zulip chat. \n",
        "\n",
        "**Note:** feel free to add more cells per section, but keep your code organised according to the section headers.\n",
        "#' \n",
        "## Code for Task 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add the code you generate for task 1 here or in any additional cells before the header for the next task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Code for Task 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add the code you generate for task 2 here or in any additional cells before the header for the next task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Code for Task 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add the code you generate for task 3 here or in any additional cells before the header for the next task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Performance profiles\n",
        "\n",
        "The functions below calculate generates the data and calculate everything that is needed to generate the performance profiles. Notice how the functions are generated. These are quadratic functions for which the matrix $A$ (its Hessian) is randomly generated in a way that the main diagonal is artificially inflated to provoke larger eigenvalues (the larger the values $\\delta$, the smaller will be the relative difference between the eigenvalues.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "using Random # for generating the instances\n",
        "\n",
        "## Generate a random symmetric positive definite matrix\n",
        "## A ∈ ℜⁿˣⁿ and a random vector b ∈ ℜⁿ\n",
        "function generate_problem_data(n::Int, δ::Float64)\n",
        "    A = randn(n,n)                # Create random matrix\n",
        "    A = (A + A')/2                # Make A symmetric\n",
        "    if isposdef(A) == false       # Check if A is PD\n",
        "        λᵢ = eigmin(A)            # Minimum eigenvalue\n",
        "        A = A + (abs(λᵢ) + δ)*I   # Add λᵢ + δ to diagonal elements\n",
        "    end\n",
        "    @assert(isposdef(A))          # Final PD test\n",
        "    b = randn(n)                  # Create random vector b\n",
        "    return (A,b)                  # Resulting matrix A is PD\n",
        "end\n",
        "\n",
        "\n",
        "# Generate k test instances of dimension n\n",
        "function generate_instances(k::Int, n::Int, δ::StepRangeLen)\n",
        "    A = Dict{Int,Matrix{Float64}}()   # Store matrices A\n",
        "    b = Dict{Int,Vector{Float64}}()   # Store vectors  b\n",
        "    for i = 1:k\n",
        "        ## NOTE: Change δ between, e.g., δ ∈ [0.01, 1] to get different\n",
        "        ##       condition numbers for matrix A\n",
        "        (A[i], b[i]) = generate_problem_data(n, δ[i])\n",
        "    end\n",
        "    return (A, b)\n",
        "end\n",
        "\n",
        "# Experiment parameters\n",
        "Random.seed!(2021)                          # Control randomness\n",
        "k = 50                                      # Number of instances to generate\n",
        "n = 100                                     # Dimension of PD matrix A ∈ ℜⁿˣⁿ\n",
        "δ1 = range(0.05, length = k, step = 0.05)   # Moderate condition numbers for matrices A\n",
        "δ2 = range(0.005, length = k, step = 0.005) # Larger condition numbers for matrices A\n",
        "\n",
        "# Resetting the parameters for the performance profiles\n",
        "# !Do not change these parameters hereafrer                                 \n",
        "N   = 10000                      # Number of iterations\n",
        "x₀  = ones(n)                    # Starting point\n",
        "ns  = 8                          # Number of solvers (methods) to compare\n",
        "np  = k                          # Number of problems to solve\n",
        "\n",
        "## Generate problem data with δ1 and δ2\n",
        "(A1, b1) = generate_instances(k, n, δ1)\n",
        "(A2, b2) = generate_instances(k, n, δ2)\n",
        "\n",
        "## Function to minimize with two different data\n",
        "f1(x,i) = (1/2)*dot(x, A1[i]*x) - dot(b1[i], x)\n",
        "f2(x,i) = (1/2)*dot(x, A2[i]*x) - dot(b2[i], x)\n",
        "\n",
        "## Optimal solution costs\n",
        "fopt = zeros(k,2)\n",
        "for i = 1:k\n",
        "    x1 = A1[i]\\b1[i]\n",
        "    x2 = A2[i]\\b2[i]\n",
        "    fopt[i,1] = f1(x1,i)\n",
        "    fopt[i,2] = f2(x2,i)\n",
        "end\n",
        "\n",
        "###### Preallocate data #######\n",
        "\n",
        "# Solution costs\n",
        "fval_gradient_exact   = zeros(k, 2)\n",
        "fval_gradient_armijo   = zeros(k, 2)\n",
        "fval_heavy_ball_exact  = zeros(k, 2)\n",
        "fval_heavy_ball_armijo = zeros(k, 2)\n",
        "fval_newton_exact      = zeros(k, 2)\n",
        "fval_newton_armijo     = zeros(k, 2)\n",
        "fval_bfgs_exact        = zeros(k, 2)\n",
        "fval_bfgs_armijo       = zeros(k, 2)\n",
        "\n",
        "# Solution times\n",
        "time_gradient_exact    = zeros(k, 2)\n",
        "time_gradient_armijo   = zeros(k, 2)\n",
        "time_heavy_ball_exact  = zeros(k, 2)\n",
        "time_heavy_ball_armijo = zeros(k, 2)\n",
        "time_newton_exact      = zeros(k, 2)\n",
        "time_newton_armijo     = zeros(k, 2)\n",
        "time_bfgs_exact        = zeros(k, 2)\n",
        "time_bfgs_armijo       = zeros(k, 2)\n",
        "\n",
        "# Number of iterations\n",
        "iter_gradient_exact    = zeros(Int, k, 2)\n",
        "iter_gradient_armijo   = zeros(Int, k, 2)\n",
        "iter_heavy_ball_exact  = zeros(Int, k, 2)\n",
        "iter_heavy_ball_armijo = zeros(Int, k, 2)\n",
        "iter_newton_exact      = zeros(Int, k, 2)\n",
        "iter_newton_armijo     = zeros(Int, k, 2)\n",
        "iter_bfgs_exact        = zeros(Int, k, 2)\n",
        "iter_bfgs_armijo       = zeros(Int, k, 2)\n",
        "\n",
        "# Solution status\n",
        "stat_gradient_exact    = fill(false, k, 2)\n",
        "stat_gradient_armijo   = fill(false, k, 2)\n",
        "stat_heavy_ball_exact  = fill(false, k, 2)\n",
        "stat_heavy_ball_armijo = fill(false, k, 2)\n",
        "stat_newton_exact      = fill(false, k, 2)\n",
        "stat_newton_armijo     = fill(false, k, 2)\n",
        "stat_bfgs_exact        = fill(false, k, 2)\n",
        "stat_bfgs_armijo       = fill(false, k, 2)\n",
        "\n",
        "computing_time = zeros(np,ns,2);  # Computing times for each problem/method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate the plot profiles\n",
        "\n",
        "The cell below perform the calculations to generate the data for comparing the methods using performance profiles. You don't have to change anything in them, but it might be worth a peak. \n",
        "\n",
        "**Be careful**: the cell below will run the eight variants of the implemented methods for 50 instances of each function. This takes a considerable while, so make sure you only run it when you are 100% sure your previous implementations are correct. Allocate about 30min-1h for these results to be available. The progress will be printed as it runs, so you can estimate how far it is from termination."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tini = time()\n",
        "## Go through all instances for both sets of data\n",
        "for j = 1:2 \n",
        "    for i = 1:k\n",
        "  \n",
        "        println(\"Solving problem $(j)/2, run $(i)/$(k). Elapsed time: $(round(time() - tini, digits=2))\")        \n",
        "        # Function to minimize\n",
        "        g1(x) = f1(x,i)\n",
        "        g2(x) = f2(x,i)\n",
        "    \n",
        "        # Gradient with exact line search\n",
        "        starttime = time()                      # Start timer\n",
        "        if j == 1\n",
        "            (fvalue, numiter) = gradient(g1, x_start=x₀, max_steps=N)\n",
        "        else\n",
        "            (fvalue, numiter) = gradient(g2, x_start=x₀, max_steps=N)\n",
        "        end\n",
        "        soltime = time() - starttime              # Solution time\n",
        "        status = abs(fvalue - fopt[i, j]) <= tol  # Check if solved or not\n",
        "        fval_gradient_exact[i, j] = fvalue       # Objective value\n",
        "        time_gradient_exact[i, j] = soltime      # Solution time\n",
        "        iter_gradient_exact[i, j] = numiter      # Iteration count\n",
        "        stat_gradient_exact[i, j] = status       # Solution status\n",
        "        # Set solution time accordingly\n",
        "        status == true ? computing_time[i, 1, j] = soltime : computing_time[i, 1, j] = Inf    \n",
        "    \n",
        "        # Gradient + Armijo\n",
        "        starttime = time()                      # Start timer\n",
        "        if j == 1\n",
        "            (fvalue, numiter) = gradient(g1, x_start=x₀, max_steps=N, exact_ls=false)\n",
        "        else\n",
        "            (fvalue, numiter) = gradient(g2, x_start=x₀, max_steps=N, exact_ls=false)\n",
        "        end\n",
        "        soltime = time() - starttime              # Solution time\n",
        "        status = abs(fvalue - fopt[i, j]) <= tol  # Check if solved or not\n",
        "        fval_gradient_armijo[i, j] = fvalue       # Objective value\n",
        "        time_gradient_armijo[i, j] = soltime      # Solution time\n",
        "        iter_gradient_armijo[i, j] = numiter      # Iteration count\n",
        "        stat_gradient_armijo[i, j] = status       # Solution status\n",
        "        ## Set solution time accordingly\n",
        "        status == true ? computing_time[i, 2, j] = soltime : computing_time[i, 2, j] = Inf  \n",
        "    \n",
        "        ## Heavy ball + Golden\n",
        "        starttime = time()                      # Start timer\n",
        "        if j == 1\n",
        "            (fvalue, numiter) = heavy_ball(g1, x_start=x₀, max_steps=N)\n",
        "        else\n",
        "            (fvalue, numiter) = heavy_ball(g2, x_start=x₀, max_steps=N)\n",
        "        end\n",
        "        soltime = time() - starttime              # Solution time\n",
        "        status = abs(fvalue - fopt[i, j]) <= tol  # Check if solved or not\n",
        "        fval_heavy_ball_exact[i, j] = fvalue     # Objective value\n",
        "        time_heavy_ball_exact[i, j] = soltime    # Solution time\n",
        "        iter_heavy_ball_exact[i, j] = numiter    # Iteration count\n",
        "        stat_heavy_ball_exact[i, j] = status     # Solution status\n",
        "        ## Set solution time accordingly\n",
        "        status == true ? computing_time[i, 3, j] = soltime : computing_time[i, 3, j] = Inf  \n",
        "    \n",
        "        ## Heavy ball + Armijo\n",
        "        starttime = time()                        # Start timer\n",
        "        if j == 1\n",
        "            (fvalue, numiter) = heavy_ball(g1, x_start=x₀, max_steps=N, exact_ls=false)\n",
        "        else\n",
        "            (fvalue, numiter) = heavy_ball(g2, x_start=x₀, max_steps=N, exact_ls=false)\n",
        "        end\n",
        "        soltime = time() - starttime              # Solution time\n",
        "        status = abs(fvalue - fopt[i, j]) <= tol  # Check if solved or not\n",
        "        fval_heavy_ball_armijo[i, j] = fvalue     # Objective value\n",
        "        time_heavy_ball_armijo[i, j] = soltime    # Solution time\n",
        "        iter_heavy_ball_armijo[i, j] = numiter    # Iteration count\n",
        "        stat_heavy_ball_armijo[i, j] = status     # Solution status\n",
        "        ## Set solution time accordingly\n",
        "        status == true ? computing_time[i, 4, j] = soltime : computing_time[i, 4, j] = Inf  \n",
        "\n",
        "        ## Newton + Golden\n",
        "        starttime = time()                        # Start timer\n",
        "        if j == 1\n",
        "            (fvalue, numiter) = newton(g1, x_start=x₀, max_steps=N)\n",
        "        else\n",
        "            (fvalue, numiter) = newton(g2, x_start=x₀, max_steps=N)\n",
        "        end\n",
        "        soltime = time() - starttime              # Solution time\n",
        "        status = abs(fvalue - fopt[i, j]) <= tol  # Check if solved or not\n",
        "        fval_newton_exact[i, j] = fvalue         # Objective value\n",
        "        time_newton_exact[i, j] = soltime        # Solution time\n",
        "        iter_newton_exact[i, j] = numiter        # Iteration count\n",
        "        stat_newton_exact[i, j] = status         # Solution status\n",
        "        ## Set solution time accordingly\n",
        "        status == true ? computing_time[i, 5, j] = soltime : computing_time[i, 5, j] = Inf\n",
        "    \n",
        "        ## Newton + Armijo\n",
        "        starttime = time()                        # Start timer\n",
        "        if j == 1\n",
        "            (fvalue, numiter) = newton(g1, x_start=x₀, max_steps=N, exact_ls=false)\n",
        "        else\n",
        "            (fvalue, numiter) = newton(g2, x_start=x₀, max_steps=N, exact_ls=false)\n",
        "        end\n",
        "        soltime = time() - starttime              # Solution time\n",
        "        status = abs(fvalue - fopt[i, j]) <= tol  # Check if solved or not\n",
        "        fval_newton_armijo[i, j] = fvalue         # Objective value\n",
        "        time_newton_armijo[i, j] = soltime        # Solution time\n",
        "        iter_newton_armijo[i, j] = numiter        # Iteration count\n",
        "        stat_newton_armijo[i, j] = status         # Solution status\n",
        "        ## Set solution time accordingly\n",
        "        status == true ? computing_time[i, 6, j] = soltime : computing_time[i, 6, j] = Inf    \n",
        "        \n",
        "        ## BFGS + Golden\n",
        "        starttime = time()                        # Start timer\n",
        "        if j == 1\n",
        "            (fvalue, numiter) = bfgs(g1, x_start=x₀, max_steps=N)\n",
        "        else\n",
        "            (fvalue, numiter) = bfgs(g2, x_start=x₀, max_steps=N)\n",
        "        end\n",
        "        soltime = time() - starttime              # Solution time\n",
        "        status = abs(fvalue - fopt[i, j]) <= tol  # Check if solved or not\n",
        "        fval_bfgs_exact[i, j] = fvalue            # Objective value\n",
        "        time_bfgs_exact[i, j] = soltime           # Solution time\n",
        "        iter_bfgs_exact[i, j] = numiter           # Iteration count\n",
        "        stat_bfgs_exact[i, j] = status            # Solution status\n",
        "        ## Set solution time accordingly\n",
        "        status == true ? computing_time[i, 7, j] = soltime : computing_time[i, 7, j] = Inf\n",
        "    \n",
        "        ## BFGS + Armijo\n",
        "        starttime = time()                        # Start timer\n",
        "        if j == 1\n",
        "            (fvalue, numiter) = bfgs(g1, x_start=x₀, max_steps=N, exact_ls=false)\n",
        "        else\n",
        "            (fvalue, numiter) = bfgs(g2, x_start=x₀, max_steps=N, exact_ls=false)\n",
        "        end\n",
        "        soltime = time() - starttime              # Solution time\n",
        "        status = abs(fvalue - fopt[i, j]) <= tol  # Check if solved or not\n",
        "        fval_bfgs_armijo[i, j] = fvalue           # Objective value\n",
        "        time_bfgs_armijo[i, j] = soltime          # Solution time\n",
        "        iter_bfgs_armijo[i, j] = numiter          # Iteration count\n",
        "        stat_bfgs_armijo[i, j] = status           # Solution status\n",
        "        ## Set solution time accordingly\n",
        "        status == true ? computing_time[i, 8, j] = soltime : computing_time[i, 8, j] = Inf\n",
        "    end    \n",
        "end\n",
        "println(\"Total time: $(time() - tini)\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plot performance profiles\n",
        "\n",
        "The code below generate the pdf files with the plots of each performance profile (one for the smaller condition numbers (condition 1) and another for the larger condition numbers (condition 2)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for j = 1:2\n",
        "    ###### Plot performance profiles ######\n",
        "    computing_time_min = minimum(computing_time[:, :, j], dims = 2)    # Minimum time for each instance\n",
        "    performance_ratios = computing_time[:, :, j] ./ computing_time_min # Compute performance ratios\n",
        "\n",
        "    τ = sort(unique(performance_ratios))  # Sort the performance ratios in increasing order\n",
        "    τ[end] == Inf && pop!(τ)  # Remove the Inf element if it exists\n",
        "\n",
        "    ns = 8                    # Number of solvers\n",
        "    np = k                    # Number of problems\n",
        "\n",
        "    ρS = Dict()               # Compute cumulative distribution functions\n",
        "    for i = 1:ns              # for performance ratios\n",
        "        ρS[i] = [sum(performance_ratios[:,i] .<= τi) / np for τi in τ]\n",
        "    end\n",
        "\n",
        "    # Plot performance profiles\n",
        "    labels = [\"Gradient (Exact)\", \"Gradient (Armijo)\", \"Heavy ball (Exact)\", \"Heavy ball (Armijo)\",\n",
        "              \"Newton (Exact)\", \"Newton (Armijo)\", \"BFGS (Exact)\", \"BFGS (Armijo)\"]\n",
        "\n",
        "    styles = [:solid, :dash, :solid, :dash, :solid, :dash, :solid, :dash,]\n",
        "    colors = [:1, :1, :2, :2, :3, :3, :4, :4]\n",
        "    plot(xscale = :log2,  \n",
        "         yscale = :none,\n",
        "         xlim   = (1, maximum(τ)),\n",
        "         ylim   = (0, 1),\n",
        "         xlabel = L\"$\\tau$\",\n",
        "         ylabel = L\"$P(r_{ps} \\leq \\tau : 1 \\leq s \\leq np)$\",\n",
        "         title  = \"Performance plot (condition $(j))\",\n",
        "         xticks = [2^i for i = 0:9],\n",
        "         yticks = 0.0:0.1:1.0,\n",
        "         size   = (900,600),\n",
        "         reuse  = false,\n",
        "         tickfontsize   = 8,\n",
        "         legendfontsize = 10,\n",
        "         guidefontsize  = 10,\n",
        "         grid = true,\n",
        "         legend=:bottomright)\n",
        "    for i = 1:ns\n",
        "      plot!(τ, ρS[i], label = labels[i], seriestype = :steppre, linewidth = 1, line = styles[i], color = colors[i])\n",
        "    end\n",
        "    savefig(\"performance_plot_$(j).pdf\")\n",
        "end"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Julia 1.6.2",
      "language": "julia",
      "name": "julia-1.6"
    },
    "language_info": {
      "file_extension": ".jl",
      "mimetype": "application/julia",
      "name": "julia",
      "version": "1.6.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
