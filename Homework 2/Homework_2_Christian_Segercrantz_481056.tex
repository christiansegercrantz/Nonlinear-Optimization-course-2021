\documentclass{article}
\usepackage[margin=3cm]{geometry}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{float}
\usepackage{enumitem}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}

\graphicspath{ {plots/} }

\title{Nonlinear Optimization - Homework 2 }
\author{Christian Segercrantz 481056}


\begin{document}
	\maketitle
	\pagebreak
\section*{2.1}
\begin{equation} \label{eq:1}
	f(x) = x_1^3-x_1+x_2^3-x_2
\end{equation}
\subsection*{(a)}
	\begin{figure}[H]
		\includegraphics[width=0.8\textwidth]{H21_surface.png}
		\caption{The surface plot of equation \eqref{eq:1}.}
		\label{fig:1a}
	\end{figure}
	By examining Figure \ref{fig:1a}, I would conclude that the function is non-convex as we can clearly see areas that do not follow the definition of a convex function $f(\lambda x_1 + (1 - \lambda)x_2 ) \leq \lambda f(x_1 ) + (1 - \lambda)f(x_2)$.
\subsection*{(b)}
	In order for us to find the critical point, we will solve the first-order derivative of the function.
	\begin{equation}
		\nabla f(x_1,x_2) =
		\begin{bmatrix}
			3x_1^2-1 \\
			3x_2^2-1
		\end{bmatrix}.
	\end{equation}
	The first-order necessary condition is $\nabla f(\bar{x}) = 0$ for our unconstrained problem . We can solve the following equation since the equations are the same to get the critical points:
	\begin{equation}
		3x^2-1 = 0 \iff x= \pm\sqrt{1/3}.
	\end{equation}
	This gives us the four points $(\sqrt{1/3},\sqrt{1/3})$, $(-\sqrt{1/3},\sqrt{1/3})$, $(\sqrt{1/3},-\sqrt{1/3})$, and $(-\sqrt{1/3},-\sqrt{1/3})$.
\subsection*{(c)}
	We calculate the hessian from the gradient and receive 
	\begin{equation}
		Hf(x) = \begin{bmatrix}
			6x_1 & 0 \\
			0 & 6x_2
		\end{bmatrix} 
	\end{equation}
	In order for us to know how the function behaves, we need to examine the semi-definiteness of the Hessian. A positive semi-definite matrix has all of its eigenvalues non-negative and vice verse for a negative semi-definite matrix for all x. Additionally, a convex(/concave) functions hessian, if twice differentiable, is positive(/negative) semi-definite. The two eigen values of the function is $\lambda_1 = 6x_1$ and $\lambda_2 = 6x_2$. We can see that only for values where both $x_1$ and $x_2$ are non-negative(/-positive) is the hessian positive(/negative) semi-definite. In other words, the function is not convex(concave). 
	
	Using Julia we can solve the eigen values in order to get the curvature around the points. The point $(\sqrt{1/3},\sqrt{1/3})$ has all positive eigen values, $(-\sqrt{1/3},-\sqrt{1/3})$ has all negative eigen values and the other two one positive and one negative. From this we can conclude that $(\sqrt{1/3},\sqrt{1/3})$ is a local minima, $(-\sqrt{1/3},-\sqrt{1/3})$ a local maxima and the two others saddle points. 
\section*{2.2}
	\begin{equation}
		f(x_1,x_2) = 2x^2_1 -x_1x_2+x_2^2 -3x_1 + e^{2x_1+x_2}
	\end{equation}
\subsection*{(a)}
	By corollary 6 of lecture 4, we know that the first-order necessary condition for unconstrained problems is that \textit{"Suppose $f : \mathbb{R}^n \rightarrow \mathbb{R}$ is differentiable at $\bar{x}$. If $\bar{x}$ is a local minimum, then $\nabla f(\bar{x}) = 0$".}	The equation to be satisfied is thus $\nabla f(\bar{x}) = 0$. We calculate $\nabla f$:
	\begin{equation}
		\nabla f =
			\begin{bmatrix}
			4x_1-x_2-3+ 2 e^{2 x_1 + x_2} \\
			-x_1+2x_2 + e^{2x_1+x_2}
			\end{bmatrix}
	\end{equation}
	For this to be a sufficient condition for optimality the function $f(x_1,x_2)$ has to be convex. Then, based on theorem 8 from lecture 4 can we have sufficient conditions. Since we know the following things:
	\begin{enumerate}
		\item Polynomials are convex,
		\item The linear combination of convex functions are convex,
	\end{enumerate}
	we can conclude that the function $f(x_1,x_2)$ is convex. We thus have the necessary and sufficient condition for optimality.
\subsection*{(b)}
	If $\bar{x}=(0,0)$ is to be a optimal point, must it satisfy $\nabla f(0,0) = \boldsymbol{0}$. 
	\begin{align}
		\nabla f =&
			\begin{bmatrix}
				4x_1-x_2-3+ 2 e^{2 x_1 + x_2} \\
				-x_1+2x_2 + e^{2x_1+x_2}
			\end{bmatrix} \\
		=& \begin{bmatrix}
				0-0-3+ 2 e^{0} \\
				-0+0 + e^{0}
			\end{bmatrix}\\
		=& \begin{bmatrix}
				-1 \\
				1
		\end{bmatrix} \neq
		\begin{bmatrix}
			0 \\
			0
		\end{bmatrix}
	\end{align}
	We can see that the point $(0,0)$ does not satisfy the condition.
	
	The direction $d$ that makes the function decrease must satisfy the condition $\nabla f(\bar{x})^\top d <0$.
\subsection*{(c)}
	To find the minimum for $f(\bar{x})$, where $\bar{x}=\begin{bmatrix}0 \\ 0\end{bmatrix}$, in the direction d= $\begin{bmatrix}1 \\ 0\end{bmatrix}$ we must calculate the step size $\bar{\lambda}=\text{argmin}_{\lambda} d^\top \nabla f(\bar{x}+ \lambda d)$. 
	\begin{align}
		\theta'(\lambda)= &\begin{bmatrix}
			1 & 0
		\end{bmatrix}
		\nabla f
		\left(\begin{bmatrix}
			x_1 \\
			x_2
		\end{bmatrix}+
		\lambda
		\begin{bmatrix}
			1 \\
			 0
		 \end{bmatrix}\right) \\
	=&
	 \begin{bmatrix}
	 	1 & 0
	 \end{bmatrix}
	 \nabla f\left(
	 \begin{bmatrix}
	 	x_1+\lambda \\
	 	x_2
	 \end{bmatrix}\right) \\
 	=&
	 \begin{bmatrix}
 		1 & 0
 	\end{bmatrix}
	\begin{bmatrix}
 		4(x_1+\lambda)-x_2 -3+ 2 e^{2 (x_1+\lambda) + x_2 } \\
 		-(x_1+\lambda)+2x_2 + e^{2(x_1+\lambda)+x_2)}
 	\end{bmatrix} \\
 	 =&
 		4\lambda-x_2 + 4x_1-3+ 2 e^{2\lambda + 2 x_1 + x_2 } \\
	\end{align}
	We substitute $(x_1,x_2)$ the point $(0,0)$ now and get
	\begin{equation}
		\theta'(\lambda)= 4\lambda +2 e^{2\lambda}.
	\end{equation}
	In order for this to be optimal we set the point zero and solve for $\lambda$:
	\begin{align}
		0 &= 4 \lambda + 2e^{2\lambda}\\
		\implies \lambda &= \frac{1}{4} (3 - 2 W(e^{3/2})) \approx 0.118,
	\end{align}
	where W is the log product function.
	
	We can now calculate the new point now by noticing that $\lambda d = \begin{bmatrix} \lambda \\ 0 \end{bmatrix}$. Since the starting point is $(0,0)$ we get the new point by simply setting $f(\lambda,0) \approx 0,94$.
	
\section*{2.3}
\begin{table}[H]
	The code can be seen in the attached jupyter notebook.
	The iterations of all three methods can be seen from Table \ref{tab:iterations}. Since they can easily been seen from the table, the answers have not been split into a-d parts.
	
	In the case of the Newton method, we are not only employing first-order derivative information but also second order derivative information. This leads to a more complex computations but need for less iterations. For the conjugate gradient, which we can see lies between the gradient and newton methods in iteration amount, we are not directly using the Hessian but still using more information than in the gradient method.
	
	For the Conjugate gradient method, 
	\centering
	\caption{The iterations for both functions f and g of exercise 2.3.}
	\label{tab:iterations}
	\begin{tabular}{|l|l|l|l|}
		\hline
		Function   & Gradient Descent & Newtonâ€™s method & Conjugate Gradient method \\ \hline
		$f(7,3)$ & 11               & 1               & 8                       \\ \hline
		$g(-4,-2)$ & 18             & 4               & 6                       \\ \hline
	\end{tabular}
\end{table}
\end{document}