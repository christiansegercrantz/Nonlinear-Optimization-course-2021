{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# MS-E2122 - Nonlinear Optimization\n### Prof. Fabricio Oliveira\n\n## Homework 2 - Problem 2.3"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "# Loading the packages we will need\nusing ForwardDiff      # Automatic differentiation\nusing LinearAlgebra    # For using norm()\nusing Test             # For implementing tests\n\n# Shorthand to the functions to compute gradient and hessian. You can use these to complete parts of the code.\n# Type \\nabla + Tab to obtain ∇. \n∇(f,x) = ForwardDiff.gradient(f, x)\nH(f,x) = ForwardDiff.hessian(f, x)\n\nconst tol = 1e-4      # unconstrained method convergence tolerance\nconst tol_ls = 1e-7   # line search convergence tolerance"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1: implementing the line search : Golden Section method\n\nYou are first required to implement the Golden section method as the exact line search to be used. Check the lecture notes for Lecture 5 for a detailed explanation of the method. \n\nAlthough it was not covered in class, this is a good opportunity for you to see if what you learn in class can help understand a new method that you haven't seen before, like it will probably be the case in your professional life!\n\n### Input parameters:\n- $\\theta$: line search function\n- $a$: initial lower bound\n- $b$: initial upper bound"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "function golden_ls(θ; a=0.0, b=10.0, l=tol_ls)\n    \n    α  = 1/MathConstants.φ        # φ = golden ratio. Here α ≈ 0.618\n    \n    λ  = a + (1-α)*(b - a)        # NOTE: We do not need to index a, b, λ, and μ like in the lecture 5 pseudocode\n    μ  = a + α*(b - a)            #       Instead, we can keep reusing and updating the same variables for notational convenience\n    \n    θμ = θ(a + α*(b - a))         # Use this variable to compute function values Θ(μₖ₊₁) as in the pseudocode of Lecture 5\n    θλ = θ(a + (1 - α)*(b - a))   # Use this variable to compute function values Θ(λₖ₊₁) as in the pseudocode of Lecture 5\n\n    # TODO: Implement what should be inside the while loop of Golden Section method. Use the variables defined above.\n    while b - a > l\n    # Include your code here, updating the values of λ and μ, \n    # testing which update to do (left or right move) and update \n    # the values of a, b, θλ, and θμ accordingly\n\n\n\n\n\n\n\n\n\n\n\n\n\n    end\n    \n    return (a + b)/2              # Finally, the function returns the center point of the final interval\nend"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test functions\n\nUse the functions below to test your code. The correct results is commented beside it.\n\n*Tip:* if you want to see the value of any expression you can use the macro `@show`.\n\nExample: \n`@show golden_ls(θ1, a, b)`"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "# Tests\nθ1(x) = 3x^2 -4x + 6           # Optimal value x = 2/3\nθ2(x) = exp(x) - 10x^2 - 20x   # Optimal value x = 4.743864 (in [0,10])\n\n@test abs(golden_ls(θ1) - 2/3) <= 1e-4\n@test abs(golden_ls(θ2) - 4.743864) <= 1e-4"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2: implementing the Gradient Descent method\n\nWe will now implement the gradient descent method. Below we list all the inputs and outputs we considered for reference.\n\n### Inputs\n- f: function to minimize (mandatory)\n- x_start: starting point\n- max_steps: maximum number of iterations\n- ϵ: convergence tolerance.\n\n### Outputs\nReturn the tuple `(x_iter, f(x), k-1)` \n- x_iter: a matrix with n columns and as many rows as iterations with each point visited\n- f(x): function value of all points visited\n- k-1: number of iterations, discounted the \"iteration 0\" for which k = 1."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "function gradient(f; x_start=[0,0], max_steps=1000, ϵ=tol)\n    \n    x = zeros(max_steps, length(x_start))                 # To save the history of iterations\n    x = vcat(x_start', x)                                 # Including starting point                           \n\n    for k = 1:max_steps                                   # Main iteration loop\n        \n        ∇f = ∇(f, x[k,:])                                 # Gradient at iteration k\n\n        if norm(∇f) < ϵ                                   # Stopping condition: norm of the gradient < tolerance     \n            \n            return (x[1:k,:], f.(x[i,:] for i=1:k), k-1)  # Return iteration points, function values, and number of iterations\n        end                                               # (k-1) as k=1 is \"iteration 0\".      \n        \n        # TODO: set the Gradient Descent direction (do not forget to normalise d)\n\n        \n        θ(λ) = f(x[k,:] + λ*d)      # Define the line search function \n        λ    = golden_ls(θ)         # Call Golden Section method to compute optimal step size λ  \n\n        # TODO: Update the solution x[k+1,:] at this iteration accordingly\n\n    \n    end\n\n    return (x, f.(x[i,:] for i=1:max_steps), max_steps)    # Return iteration points, function values, and number of iterations\nend"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing the gradient descent method\n\nThe cell below implement tests to validate your implementation of the gradient method. We use the same functions for the other methods too."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "# Test functions \nf(x) = 0.26*(x[1]^2 + x[2]^2) - 0.48*x[1]*x[2]  # x_opt = (0,0) and f(x_opt) = 0.\ng(x) = exp(x[1] + 3*x[2] - 0.1) + exp(x[1] - 3*x[2] - 0.1) + exp(-x[1] - 0.1) # x_opt ≈ (-0.346574, 0.0) and f(x_opt) = 2.55927\n                     \n# Testing f(x)\nx = [7.0, 3.0]                        # Starting point (7.0,3.0)\n(xg, fg, kg) = gradient(f, x_start=x)\n@test norm(xg[end,:] - [0.0, 0.0]) <= 1e-2\n@test abs(fg[end] - 0.0) <= 1e-2\n      \n# Testing g(x)\nx = [-4.0, -2.0]                      # Starting point (-4.0,-2.0)\n(xg, fg, kg) = gradient(g, x_start=x)\n@test norm(xg[end,:] - [-0.346574, 0.0]) <= 1e-2\n@test abs(fg[end] - 2.55927) <= 1e-2"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can recover the elements of the solution by calling the function as above and then printing (or showing with `@show`) each element."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "(xg, fg, kg) = gradient(f, x_start=x)\nprintln(\"Optimal solution: $(xg[end,:])\")\nprintln(\"Optimal value: $(fg[end])\")\nprintln(\"Total iterations:  $(kg)\")"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3: Newton's method\n\n### Inputs\n- f: function to minimize (mandatory)\n- x_start: starting point\n- max_steps: maximum number of iterations\n- ϵ: convergence tolerance.\n\n### Outputs\nReturn the tuple `(x_iter, f(x), k-1)` \n- x_iter: a matrix with n columns and as many rows as iterations with each point visited\n- f(x): function value of all points visited\n- k-1: number of iterations, discounted the \"iteration 0\" for which k = 1."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "function newton(f; x_start=[0,0], max_steps=1000, ϵ=tol)\n    \n    x = zeros(max_steps, length(x_start))   # To save the history of iterations\n    x = vcat(x_start', x)                   # Including starting point \n    \n    for k = 1:max_steps                     # Main iteration loop\n        \n        ∇f = ∇(f, x[k,:])                   # Gradient at iteration k\n        \n        if norm(∇f) < ϵ                                      # Stopping condition: norm of the gradient < tolerance\n            return (x[1:k,:], f.(x[i,:] for i = 1:k), k-1)   # Return iteration points, function values, and number of iterations\n        end\n        \n        # TODO: Update the newton direction\n\n        \n        θ(λ) = f(x[k,:] + λ*d)     # Define the line search function \n        λ = golden_ls(θ)           # Call Golden Section method to compute optimal step size λ  \n\n        # TODO: Update the solution x[k+1,:] at this iteration accordingly\n\n    end\n    \n    return (x, f.(x[i,:] for i = 1:max_steps), max_steps)    # Return iteration points, function values, and number of iterations\nend"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing the Newton's method"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "# Test functions (everything is the same as before, expect for the functin newton() call)\nf(x) = 0.26*(x[1]^2 + x[2]^2) - 0.48*x[1]*x[2]  # x_opt = (0,0) and f(x_opt) = 0.\ng(x) = exp(x[1] + 3*x[2] - 0.1) + exp(x[1] - 3*x[2] - 0.1) + exp(-x[1] - 0.1) # x_opt ≈ (-0.346574, 0.0) and f(x_opt) = 2.55927\n\n# Testing f(x)\nx = [7.0, 3.0]                 # Starting point (7.0,3.0)\n(xg, fg, kg) = newton(f, x_start=x)\n@test norm(xg[end,:] - [0.0, 0.0]) <= 1e-2\n@test abs(fg[end] - 0.0) <=1e-2\n      \n# Testing g(x)\nx = [-4.0, -2.0]               # Starting point (-4.0,-2.0)\n(xg, fg, kg) = newton(g, x_start=x)\n@test norm(xg[end,:] - [-0.346574, 0.0]) <= 1e-2\n@test abs(fg[end] - 2.55927) <=1e-2"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conjugate Gradient\n\n### Inputs\n- f: function to minimize (mandatory)\n- x_start: starting point\n- max_steps: maximum number of iterations\n- ϵ: convergence tolerance.\n\n### Outputs\nReturn the tuple `(x_iter, f(x), k-1)` \n- x_iter: a matrix with n columns and as many rows as iterations with each point visited\n- f(x): function value of all points visited\n- k-1: number of iterations, discounted the \"iteration 0\" for which k = 1."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "function conjugate_gradient(f; x_start=[0,0], max_steps=1000, ϵ=tol)\n      \n    α = 0.0              # Coefficient for Fletcher-Reeves update\n    k = 1                # Iteration number  \n    n = length(x_start)  # Dimension of x\n    d = -∇(f, x_start)   # Initial direction vector\n\n    x = zeros(max_steps, length(x_start))   # To save the history of iterations\n    x = vcat(x_start', x)                   # Including starting point \n    \n    while k <= max_steps   # Go through max iterations N and return if at optimum \n        \n        for j = 1:n        # Go through each element of x. NOTE: We do not need to use y variables. Instead, \n                           # we can use the empty values in the x variable vector \n\n            θ(λ) = f(x[k,:] + λ*d)   # Define the line search function \n            λ = golden_ls(θ)   # Call Golden Section method to compute optimal step size λ  \n            \n            # TODO: Update the value of x[k+1,:] accordingly\n\n\n            # TODO: Compute value of α using the Fletcher-Reeves update formula\n\n\n            # TODO: Set the direction vector accordingly\n\n            \n            k = k + 1   # Update number of iterations for the y values (here we use x vector instead as mentioned earlier)\n            \n        end\n        \n        d = -∇(f, x[k,:]) # Setting d to the gradient for the next cycle of iterations \n        \n        if norm(d) < ϵ                                      # Stopping condition: norm of the gradient < tolerance\n            return (x[1:k,:], f.(x[i,:] for i = 1:k), k-1)  # Return iteration points, function values, and number of iterations\n        end\n        \n    end\n    \n    return (x, f.(x[i,:] for i = 1:max_steps), max_steps)    # Return iteration points, function values, and number of iterations\n    \nend"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing the conjugate gradient method"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "# Test functions (everything is the same as before, expect for the functin conjugate_gradient() call)\nf(x) = 0.26*(x[1]^2 + x[2]^2) - 0.48*x[1]*x[2]  # x_opt = (0,0) and f(x_opt) = 0.\ng(x) = exp(x[1] + 3*x[2] - 0.1) + exp(x[1] - 3*x[2] - 0.1) + exp(-x[1] - 0.1) # x_opt ≈ (-0.346574, 0.0) and f(x_opt) = 2.55927\n\n# Testing f(x)\nx = [7.0, 3.0]                                      # Starting point (7.0,3.0)\n(xg, fg, kg) = conjugate_gradient(f, x_start=x)\n@test norm(xg[end,:] - [0.0, 0.0]) <= 1e-2\n@test abs(fg[end] - 0.0) <=1e-2\n      \n# Testing g(x)\nx = [-4.0, -2.0]                                    # Starting point (-4.0,-2.0) \n(xg, fg, kg) = conjugate_gradient(g, x_start=x)\n@test norm(xg[end,:] - [-0.346574, 0.0]) <= 1e-2\n@test abs(fg[end] - 2.55927) <=1e-2"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plotting the convergence trajectories of the methods\n\nIf the above implementations were done correctly, the code below should generate a similar picture to those used in the classes. We will be using your implementaion of the methods to calculate the optimal trajectories and plot the three methods side by side.\n\nTip: You can use this code as a reference to make plots in the future assignments."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using Plots \nusing LaTeXStrings   # For plotting LaTeX code in tags\npyplot()\n\nf(x) = exp(-(x[1]-3)/2) + exp((4x[2] + x[1])/10) + exp((-4x[2] + x[1])/10) # standard function\n# Plotting the contours of the function to be optimised\nn = 1000\nx1 = range(-10, 25, length=n);\nx2 = range(-10, 10, length=n);\n\ncontour(x1, x2, (x1,x2) -> f([x1,x2]),\n        levels = [3.6, 4, 5, 6 , 8, 10, 15, 20, 30], # set which level curves to plot\n        xaxis = (L\"$x_1$\", (-5,15)),                 # L means we are usign LaTeX notation\n        yaxis = (L\"$x_2$\", (-5,5)),\n        title = L\"$f(x) = e^{-(x_1-3)/2} + e^{(4x_2 + x_1)/10} + e^{(-4x_2 + x_1)/10}$\",  \n        clims = (0,35),\n        contour_labels = true,\n        cbar = false,\n        aspect_ratio = :equal\n        )\n\n# The optimal value of x for standard function:\nxopt = [-(5/6)*(-3 + 2*log(2) - 2*log(5)), 0]\nfopt = f(xopt)\n\n# Testing f(x)\nx = [12.0, -2.0]\n(xg,⋅,⋅) = gradient(f, x_start=x) # We use \\cdot to discard outputs we do not need\n(xn,⋅,⋅) = newton(f, x_start=x)\n(xc,⋅,⋅) = conjugate_gradient(f, x_start=x)\n\nplot!( xg[:,1], xg[:,2], label = \"Gradient\", marker=:circle)\nplot!( xn[:,1], xn[:,2], label = \"Newton\", marker=:circle)\nplot!( xc[:,1], xc[:,2], label = \"Conj. grad.\", marker=:circle)"
      ],
      "metadata": {},
      "execution_count": null
    }
  ],
  "nbformat_minor": 2,
  "metadata": {
    "language_info": {
      "file_extension": ".jl",
      "mimetype": "application/julia",
      "name": "julia",
      "version": "1.5.3"
    },
    "kernelspec": {
      "name": "julia-1.5",
      "display_name": "Julia 1.5.3",
      "language": "julia"
    }
  },
  "nbformat": 4
}
